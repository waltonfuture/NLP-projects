MTBart modifies the decoder of BART to enhance its natural language understanding capabilities. Five different types of decoders streams, designed by changing the attention masks and input ids, replace the initial BART decoder. The model is pretrained on wiki-text and book corpus, and fine-tuned on GLUE, SquaD, etc.


![mtbart1.png](https://img1.imgtp.com/2023/05/11/yKN6EafU.png)

![mtbart2.png](https://img1.imgtp.com/2023/05/11/xHbWaPho.png)

